{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7719e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920233b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of states in grid:  9\n",
      "No. of action options in each state: 4\n"
     ]
    }
   ],
   "source": [
    "#defining action variables\n",
    "up=0\n",
    "down=1\n",
    "left=2\n",
    "right=3\n",
    "\n",
    "#no. of states and no. of variables\n",
    "noS=3*3\n",
    "noA=4\n",
    "\n",
    "S=range(noS)\n",
    "\n",
    "reward =-1    #for every step\n",
    "\n",
    "terminal_state = lambda s: s==0  #first and last state terminal\n",
    "\n",
    "wall=[]\n",
    "\n",
    "P=dict()  # transition probabilities\n",
    "\n",
    "# In the dictionary of transition dynamics P[state][action] gives us a tupple which tells us\n",
    "# the (probability of transition to the next state,\n",
    "#       next state, \n",
    "#       reward the environment gives for the transition,\n",
    "#       if the environment is done )  \n",
    "\n",
    "# Hard coding this environment P\n",
    "\n",
    "for s in S:\n",
    "    P[s]=dict()\n",
    "    \n",
    "    if (terminal_state(s)):\n",
    "        P[s][up]=(1.0,s,0.0,True)   # next_state, probability, reward\n",
    "        P[s][down]=(1.0,s,0.0,True)\n",
    "        P[s][right]=(1.0,s,0.0,True)\n",
    "        P[s][left]=(1.0,s,0.0,True)\n",
    "        \n",
    "    elif s == 1:\n",
    "        P[s][up]= (1.0, 1, -1.0 ,False) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 4, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 2, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 0, -1.0 ,True)\n",
    "        \n",
    "    elif s == 2:\n",
    "        P[s][up]= (1.0, 2, -1.0 ,False) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 5, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 2, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 1, -1.0 ,False)  \n",
    "        \n",
    "    elif s == 3:\n",
    "        P[s][up]= (1.0, 0, -1.0 ,True) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 6, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 4, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 3, -1.0 ,False) \n",
    "     \n",
    "    elif s == 4:\n",
    "        P[s][up]= (1.0, 1, -1.0 ,False) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 7, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 5, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 3, -1.0 ,False) \n",
    "        \n",
    "    elif s == 5:\n",
    "        P[s][up]= (1.0, 2, -1.0 ,False) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 8, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 5, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 4, -1.0 ,False) \n",
    "     \n",
    "    elif s == 6:\n",
    "        P[s][up]= (1.0, 3, -1.0 ,False) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 6, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 7, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 6, -1.0 ,False) \n",
    "        \n",
    "    elif s == 7:\n",
    "        P[s][up]= (1.0, 4, -1.0 ,False) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 7, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 8, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 6, -1.0 ,False) \n",
    "        \n",
    "    elif s == 8:\n",
    "        P[s][up]= (1.0, 5, -1.0 ,False) # next_state, probability, reward\n",
    "        P[s][down]=(1.0, 8, -1.0 ,False)\n",
    "        P[s][right]=(1.0, 8, -1.0 ,False)\n",
    "        P[s][left]=(1.0, 7, -1.0 ,False)     \n",
    "        \n",
    "print ('No. of states in grid: ', noS)\n",
    "print ('No. of action options in each state:', noA  ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a6844d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 6, -1.0, False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[3][1] # Looking at the transition tupple for third state and taking the 1st action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001b5998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = np.ones([noS, noA]) / noA # random policy is same as taking each action with equal probability\n",
    "random_policy   # probabilities of taking respective actions(colums) for respective states (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79b87bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: (1.0, 0, 0.0, True),\n",
       "  1: (1.0, 0, 0.0, True),\n",
       "  3: (1.0, 0, 0.0, True),\n",
       "  2: (1.0, 0, 0.0, True)},\n",
       " 1: {0: (1.0, 1, -1.0, False),\n",
       "  1: (1.0, 4, -1.0, False),\n",
       "  3: (1.0, 2, -1.0, False),\n",
       "  2: (1.0, 0, -1.0, True)},\n",
       " 2: {0: (1.0, 2, -1.0, False),\n",
       "  1: (1.0, 5, -1.0, False),\n",
       "  3: (1.0, 2, -1.0, False),\n",
       "  2: (1.0, 1, -1.0, False)},\n",
       " 3: {0: (1.0, 0, -1.0, True),\n",
       "  1: (1.0, 6, -1.0, False),\n",
       "  3: (1.0, 4, -1.0, False),\n",
       "  2: (1.0, 3, -1.0, False)},\n",
       " 4: {0: (1.0, 1, -1.0, False),\n",
       "  1: (1.0, 7, -1.0, False),\n",
       "  3: (1.0, 5, -1.0, False),\n",
       "  2: (1.0, 3, -1.0, False)},\n",
       " 5: {0: (1.0, 2, -1.0, False),\n",
       "  1: (1.0, 8, -1.0, False),\n",
       "  3: (1.0, 5, -1.0, False),\n",
       "  2: (1.0, 4, -1.0, False)},\n",
       " 6: {0: (1.0, 3, -1.0, False),\n",
       "  1: (1.0, 6, -1.0, False),\n",
       "  3: (1.0, 7, -1.0, False),\n",
       "  2: (1.0, 6, -1.0, False)},\n",
       " 7: {0: (1.0, 4, -1.0, False),\n",
       "  1: (1.0, 7, -1.0, False),\n",
       "  3: (1.0, 8, -1.0, False),\n",
       "  2: (1.0, 6, -1.0, False)},\n",
       " 8: {0: (1.0, 5, -1.0, False),\n",
       "  1: (1.0, 8, -1.0, False),\n",
       "  3: (1.0, 8, -1.0, False),\n",
       "  2: (1.0, 7, -1.0, False)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be69d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_1 = np.zeros([noS,noA]) # making a policy which takes only the second action which is going left from all states\n",
    "policy_1[:,2] =1\n",
    "policy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826201d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, discount_factor=1.0, theta=0.00001):\n",
    "     # Start with a random (all 0) value function\n",
    "    V = np.zeros(noS)\n",
    "    while True:\n",
    "        # TODO: Implement!\n",
    "        delta = 0 \n",
    "        # keep track of this to know when the while loop should end , i.e. the change per loop of a state value goes below theta\n",
    "        \n",
    "        for s in range(noS): # looping for all states\n",
    "            v= 0 \n",
    "            #initializing v = 0\n",
    "            \n",
    "            # Getting the index and action_probability for each state \n",
    "            \n",
    "            for index, action_probability in enumerate(policy[s]):\n",
    "                prob,state,reward,done = P[s][index]\n",
    "                # using the Bellman equation \n",
    "                v += prob*action_probability*(reward + discount_factor * V[state])\n",
    "                \n",
    "            # Calculating the delta \n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v    # updating the value of that state s\n",
    "            #print(V)\n",
    "        if delta < theta: # checking if the policy eval has concluded \n",
    "            break\n",
    "       \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb8f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing policy eval with k loops\n",
    "# we dont use while, but use for loop with k iterations \n",
    "\n",
    "def policy_eval_limited(policy, discount_factor=1.0, theta=0.00001, k=20):\n",
    "     # Start with a random (all 0) value function\n",
    "    V = np.zeros(noS)\n",
    "    for i in range(k):\n",
    "        # TODO: Implement!\n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(noS):\n",
    "            v= 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                prob,state,reward,done = P[s][a]\n",
    "                v += prob*action_prob*(reward + discount_factor * V[state])\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "            #print(V)\n",
    "        #if delta < theta:\n",
    "            #break\n",
    "       \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0c38532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -15.99916512, -22.49881453],\n",
       "       [-15.99916512, -21.49889629, -24.99871601],\n",
       "       [-22.49881453, -24.99871601, -26.9986209 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494c0411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -15.99916512, -22.49881453, -15.99916512,\n",
       "       -21.49889629, -24.99871601, -22.49881453, -24.99871601,\n",
       "       -26.9986209 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(random_policy, discount_factor=1.0, theta = 0.0001)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f579827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -15.99916512, -22.49881453, -15.99916512,\n",
       "       -21.49889629, -24.99871601, -22.49881453, -24.99871601,\n",
       "       -26.9986209 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = policy_eval(random_policy, discount_factor=1.0, theta = 0.0001)  \n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0443ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  -1.,  -2., -14., -15., -16., -14., -15., -16.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V2 = policy_eval_limited(policy_1, discount_factor=1.0, theta = 0.001, k=14)\n",
    "V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5da25a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_limited_random = policy_eval_limited(random_policy, discount_factor=1.0, theta = 0.001, k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c92ea838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -14.87833618, -20.90731927],\n",
       "       [-14.87833618, -20.01716505, -23.27496786],\n",
       "       [-20.90731927, -23.27496786, -25.14718054]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_limited_random.reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2e09388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-26.99862089934679"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb99e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v(second state) = action_prob * probability of transition * ( reward + discount_factor* V(prev))\n",
    "v_ex              = 0.25 * 1.0 * (-1 + 1.0*0)\n",
    "# v               = 0.25 * 1.0 * ( -1 + 1.0*0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f361c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1875"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6f1c40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  -1.,  -2., -14., -15., -16., -14., -15., -16.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "186bc41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now doing policy iteration also known as policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a43766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([noS, noA]) / noA\n",
    "    \n",
    "    # A greedy policy is a deterministic policy i.e You have 1 as action probability for some action in every state [not confused]\n",
    "    # A greedy policy is calculated after we have done policy evaluation i.e. once we know a better way to move around in the grid\n",
    "        \n",
    "    def greedy_pol(v):     # evaluating for some value function\n",
    "        pol = []           # initialize the pol list , which has the index of actions \n",
    "        for s in range(9):\n",
    "            m = []         # a list of values of next states \n",
    "            for a in range(4):\n",
    "                val = v[P[s][a][1]] \n",
    "                m.append(val)\n",
    "            action = np.argmax(np.array(m))  # finding the action which gets maximum value (action recieving max reward)\n",
    "            pol.append(action)\n",
    "         \n",
    "        policy_final = np.zeros([noS, noA]) #initialize the final policy with zeros\n",
    "        for ind,value in enumerate(pol):\n",
    "           # print(ind,value)\n",
    "            policy_final[ind][value] = 1 # assigning 1 i.e. acting greedily wrt to the optimized value function\n",
    "        return policy_final    \n",
    "    \n",
    "    # Now performing this policy eval and getting greedy policy in loops gives us the policy improvement algorithm\n",
    "    \n",
    "    pol_new = policy\n",
    "    prev_pol = np.zeros([noS, noA])\n",
    "    \n",
    "    '''\n",
    "    #doing it for j iterations \n",
    "    j = 10\n",
    "    for k in range(j):\n",
    "        # Implement this!\n",
    "        vnew = policy_eval(pol_new,discount_factor=1, theta=0.0001)\n",
    "        pol_new = greedy_pol(vnew)\n",
    "        #if prev_pol == pol_new :\n",
    "        #    break\n",
    "            \n",
    "       # prev_pol = pol_new\n",
    "        #new_pol = act_greedily arg_max of options \n",
    "    '''\n",
    "     # doing it with escape condition\n",
    "    while True: \n",
    "        # Implement this!\n",
    "        vnew = policy_eval(pol_new,discount_factor=1, theta=0.0001)\n",
    "        pol_new = greedy_pol(vnew)\n",
    "        print(pol_new)\n",
    "        #print(np.argmax(pol_new[0]))\n",
    "        #print(np.argmax(prev_pol[0]))\n",
    "        \n",
    "        # breaking when update per loop is very small \n",
    "        \n",
    "        if np.argmax(pol_new) == np.argmax(prev_pol) :\n",
    "            break\n",
    "        \n",
    "            \n",
    "        prev_pol = pol_new     # updating the prev policy to compare in the next loop\n",
    "        \n",
    "        #new_pol = act_greedily arg_max of options   \n",
    "        \n",
    "           \n",
    "    \n",
    "    return pol_new, vnew # returning both the optimal policy and the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32da3a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a,b = policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5823b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16771143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbb94c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -15.99916512, -22.49881453, -15.99916512,\n",
       "       -21.49889629, -24.99871601, -22.49881453, -24.99871601,\n",
       "       -26.9986209 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c445dd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5ba0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ao = [0,2,2,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bc9ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.706117655849084\n",
      "-11.534311711089686\n",
      "-1.0\n",
      "-11.722849559155293\n"
     ]
    }
   ],
   "source": [
    "print((v_limited_random[P[1][0][1]] + -1) )\n",
    "print((v_limited_random[P[1][1][1]] + -1) )\n",
    "print((v_limited_random[P[1][2][1]] + -1) )\n",
    "print((v_limited_random[P[1][3][1]] + -1) )\n",
    "\n",
    "m = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec30caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes policy evaluation step for only one iteration\n",
    "# i.e. value evaluation limited with k=1\n",
    "\n",
    "def value_iteration(policy_eval_fn=policy_eval, discount_factor=1.0): \n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([noS, noA]) / noA\n",
    "    \n",
    "    # A greedy policy is a deterministic policy i.e You have 1 as action probability for some action in every state [not confused]\n",
    "    # A greedy policy is calculated after we have done policy evaluation i.e. once we know a better way to move around in the grid\n",
    "        \n",
    "    def greedy_pol(v):     # evaluating for some value function\n",
    "        pol = []           # initialize the pol list , which has the index of actions \n",
    "        for s in range(9):\n",
    "            m = []         # a list of values of next states \n",
    "            for a in range(4):\n",
    "                val = v[P[s][a][1]] \n",
    "                m.append(val)\n",
    "            action = np.argmax(np.array(m))  # finding the action which gets maximum value (action recieving max reward)\n",
    "            pol.append(action)\n",
    "         \n",
    "        policy_final = np.zeros([noS, noA]) #initialize the final policy with zeros\n",
    "        for ind,value in enumerate(pol):\n",
    "           # print(ind,value)\n",
    "            policy_final[ind][value] = 1 # assigning 1 i.e. acting greedily wrt to the optimized value function\n",
    "        return policy_final    \n",
    "    \n",
    "    # Now performing this policy eval and getting greedy policy in loops gives us the policy improvement algorithm\n",
    "    \n",
    "    pol_new = policy\n",
    "    prev_pol = np.zeros([noS, noA])\n",
    "    \n",
    "    '''\n",
    "    #doing it for j iterations \n",
    "    j = 10\n",
    "    for k in range(j):\n",
    "        # Implement this!\n",
    "        vnew = policy_eval(pol_new,discount_factor=1, theta=0.0001)\n",
    "        pol_new = greedy_pol(vnew)\n",
    "        #if prev_pol == pol_new :\n",
    "        #    break\n",
    "            \n",
    "       # prev_pol = pol_new\n",
    "        #new_pol = act_greedily arg_max of options \n",
    "    '''\n",
    "     # doing it with escape condition\n",
    "    while True: \n",
    "        # Implement this!\n",
    "        vnew = policy_eval_limited(pol_new,discount_factor=1, theta=0.0001, k=1) # only change the k to be one i.e update each loop\n",
    "        pol_new = greedy_pol(vnew)\n",
    "        #print(np.argmax(pol_new[0]))\n",
    "        #print(np.argmax(prev_pol[0]))\n",
    "        \n",
    "        # breaking when update per loop is very small \n",
    "        \n",
    "        if np.argmax(pol_new) == np.argmax(prev_pol) :\n",
    "            break\n",
    "        \n",
    "            \n",
    "        prev_pol = pol_new     # updating the prev policy to compare in the next loop\n",
    "        \n",
    "        #new_pol = act_greedily arg_max of options   \n",
    "        \n",
    "           \n",
    "    \n",
    "    return pol_new, vnew # returning both the optimal policy and the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27fe60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58cd3f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.     , -1.     , -1.25   , -1.     , -1.5    , -1.6875 ,\n",
       "       -1.25   , -1.6875 , -1.84375])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3e73573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf697a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5057538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
